version: '3.8'

x-airflow-common:
  # Используем кастомный образ Airflow с ML библиотеками (XGBoost, MLflow, Kafka)
  &airflow-common
  build:
    context: ./SRC/airflow
    dockerfile: Dockerfile
  image: airflow_training:latest
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # S3 / MinIO Connection
    AIRFLOW__AWS__AWS_ACCESS_KEY_ID: minioadmin
    AIRFLOW__AWS__AWS_SECRET_ACCESS_KEY: minioadmin
    AIRFLOW__AWS__ENDPOINT_URL: http://minio:9000
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config.yml:/opt/airflow/config.yml:ro
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_MULTIPLE_DATABASES: mlflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_multiple_dbs.sh:/docker-entrypoint-initdb.d/init_multiple_dbs.sh
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 5s
      retries: 5
    restart: always
    networks:
      - fraud_detection

  redis:
    image: redis:latest
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always
    networks:
      - fraud_detection

  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: always
    networks:
      - fraud_detection

  mc:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    volumes:
      - ./wait_for_it.sh:/wait_for_it.sh
    entrypoint: /bin/bash
    command: -c "chmod +x /wait_for_it.sh && /wait_for_it.sh minio 9000 mc alias set myminio http://minio:9000 minioadmin minioadmin && mc mb myminio/mlflow || true"
    networks:
      - fraud_detection

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud_detection

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud_detection

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 30s
      retries: 5
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      DUMB_INIT_SETSID: "0"
    restart: always
    deploy:
      replicas: 2
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud_detection

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"' ]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud_detection

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # IAM-style init
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you should set AIRFLOW_UID to your user id"
          echo "otherwise files will be owned by root."
          echo "For example: echo AIRFLOW_UID=$$(id -u) > .env"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    user: "0:0"
    volumes:
      - ./dags:/sources/dags
      - ./logs:/sources/logs
      - ./plugins:/sources/plugins
    networks:
      - fraud_detection

  flower:
    <<: *airflow-common
    command: celery flower
    ports:
      - "5555:5555"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:5555/" ]
      interval: 30s
      timeout: 30s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - fraud_detection

  # MLflow Tracking Server - для отслеживания экспериментов ML и управления моделями
  mlflow:
    # Используем кастомный образ MLflow с поддержкой PostgreSQL и MinIO
    build:
      context: ./SRC/mlflow
      dockerfile: Dockerfile
    image: mlflow_server:latest
    ports:
      - "5000:5000"
    environment:
      # Backend store - PostgreSQL для хранения метаданных экспериментов
      MLFLOW_BACKEND_STORE_URI: postgresql://airflow:airflow@postgres/mlflow
      # Artifact store - MinIO (S3-совместимое хранилище) для артефактов и моделей
      MLFLOW_ARTIFACT_ROOT: s3://mlflow/
      # MinIO credentials
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command:
      - mlflow
      - server
      - --host
      - "0.0.0.0"
      - --port
      - "5000"
      - --backend-store-uri
      - postgresql://airflow:airflow@postgres/mlflow
      - --default-artifact-root
      - s3://mlflow/
    volumes:
      - ./config.yml:/mlflow/config.yml:ro
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: always
    networks:
      - fraud_detection

  # Zookeeper - координатор для Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - fraud_detection
    restart: always

  # Kafka - брокер сообщений для потоковой обработки транзакций
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - fraud_detection
    restart: always

  # Producer - генератор синтетических транзакций с маркировкой мошенничества
  producer:
    build:
      context: ./SRC/producer
      dockerfile: Dockerfile
    image: transaction_producer:latest
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: transactions
      BATCH_SIZE: 100
      BATCH_DELAY_SECONDS: 1.0
      SEED: 42
    depends_on:
      - kafka
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    restart: always
    networks:
      - fraud_detection

  inference:
    build:
      context: ./SRC/inference
      dockerfile: Dockerfile
    image: realtime_inference:latest
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: transactions
      OUTPUT_TOPIC: fraud_predictions
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MODEL_NAME: fraud_detection_xgboost
      BATCH_SIZE: 128
      BATCH_WAIT_SECONDS: 1.0
    depends_on:
      kafka:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    restart: always
    networks:
      - fraud_detection

  relay:
    build:
      context: ./SRC/relay
      dockerfile: Dockerfile
    image: fraud_relay:latest
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      OUTPUT_TOPIC: fraud_predictions
      WS_HOST: 0.0.0.0
      WS_PORT: 8000
    ports:
      - "8000:8000"
    depends_on:
      kafka:
        condition: service_healthy
    restart: always
    networks:
      - fraud_detection

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    image: fraud_frontend:latest
    environment:
      NEXT_PUBLIC_WS_URL: ws://relay:8000
    ports:
      - "3000:3000"
    depends_on:
      relay:
        condition: service_healthy
    restart: always
    networks:
      - fraud_detection

networks:
  fraud_detection:
    driver: bridge

volumes:
  postgres_data:
  minio_data:
